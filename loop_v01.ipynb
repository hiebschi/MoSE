{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiebschi/MoSE_scripts/blob/main/loop_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQ_Gx_rrKju"
      },
      "source": [
        "\n",
        "# Implementing a Segmentation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMZ_BMRnge2X"
      },
      "source": [
        "## 1. Preparations\n",
        "### 1.1 Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZhEfhwXBge2Y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import sklearn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "id": "yCfa0JGpElrw",
        "outputId": "bf8211df-73bd-4110-bd4b-e5f8b5152d8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "torchvision version: 0.20.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Install segmentation model"
      ],
      "metadata": {
        "id": "3RTROJOlxwZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAdkr3mEge2a",
        "outputId": "7fdf64dd-67f8-4262-cac1-8c945b826932",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.26.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (11.0.0)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.17.0)\n",
            "Requirement already satisfied: timm==0.9.7 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.9.7)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.5.1+cu121)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1xI34Hl2ge2a"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Device agnostic code"
      ],
      "metadata": {
        "id": "paG6kc2OyBYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8whgTmUHyEq6",
        "outputId": "e6ba75a8-1213-4749-a993-ee27e093f4e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1Db8WbBge2a"
      },
      "source": [
        "### 1.4 Upload data to colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manual upload (for small files)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FwCYsWKkDZKE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCESS TO GOOGLE DRIVE\n",
        "################################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FTZJtueDoWw",
        "outputId": "2ad1392f-ac33-4f14-97b3-ffa385f5fc8c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N_bWdgKqge2a"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Data directories on GOOGLE DRIVE  # -------------------->> ADJUSTABLE\n",
        "################################################################################\n",
        "# npy preprocessed patches\n",
        "preprocessed_patches_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/preprocessed_patches'\n",
        "# JSON class codes\n",
        "codes_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/classes'\n",
        "# Shapefile class labels\n",
        "labels_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/shapefiles'\n",
        "# npy Masks\n",
        "masks_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/masks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1kdiZmage2b",
        "outputId": "a803d594-eaae-489e-fe00-c70d64eeef6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Einzelstein' 'Wurzelstock' 'Steinverbauung' 'Totholz' 'Steinriegel'\n",
            " 'Schotterbank' 'Schlamm_Sandinsel' 'Sand_Schlammbank' 'Schotterinsel']\n"
          ]
        }
      ],
      "source": [
        "# Load class labels\n",
        "################################################################################\n",
        "# specific Shapefile path\n",
        "shp_path = os.path.join(labels_dir, \"GSK_24_WGS84_adjusted.shp\") # -------------------->> ADJUSTABLE\n",
        "\n",
        "labels = gpd.read_file(shp_path) # read shapefile\n",
        "labels_filtered = labels[labels[\"Elementtyp\"].notnull()] # remove NULL\n",
        "print(labels_filtered[\"Elementtyp\"].unique()) # print all label classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFxyHS__ge2b",
        "outputId": "ca2db6d6-7917-41ce-b2a2-0dab14bab356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Einzelstein': 1, 'Wurzelstock': 2, 'Steinverbauung': 3, 'Totholz': 4, 'Steinriegel': 5, 'Schotterbank': 6, 'Schlamm_Sandinsel': 7, 'Sand_Schlammbank': 8, 'Schotterinsel': 9}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Load class codes\n",
        "################################################################################\n",
        "# path\n",
        "label_codes_path = os.path.join(codes_dir, \"label_codes.json\")   # -------------------->> ADJUSTABLE\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(label_codes_path, \"r\") as json_file:\n",
        "    label_codes = json.load(json_file)\n",
        "\n",
        "print(label_codes)\n",
        "len(label_codes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9K2zVwg6HeaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reversed dictionary\n",
        "reversed_label_codes = {v: k for k, v in label_codes.items()} # v = value, k = key\n",
        "reversed_label_codes[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CtVjFKrwEJht",
        "outputId": "75de0fa0-374c-4c3b-8add-2e9b33611d10"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Totholz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Load preprocessed patches\n",
        "################################################################################\n",
        "\n",
        "# list of all .npz-files (.npz-compression)\n",
        "################################################################################\n",
        "patches_npz_list = [f for f in os.listdir(preprocessed_patches_dir) if f.endswith('.npz')]\n",
        "patches_npz_list.sort()\n",
        "patches_npz_list[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISqNiRoew3wM",
        "outputId": "a7f76254-d692-45e9-d011-7ecbb6e9ebb8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A01_patch_0.npy.npz', 'A01_patch_1.npy.npz', 'A01_patch_10.npy.npz']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ################################################################################\n",
        "# # SLOW VERSION TO LOAD ALL PATCHES (arrays)\n",
        "# ################################################################################\n",
        "\n",
        "# # empty list for saving loaded arrays (patches)\n",
        "# patches = []\n",
        "\n",
        "# for idx, patch_npz_name in enumerate(patches_npz_list): # iterates over all npz-patches in the list\n",
        "#   patch_npz_path = os.path.join(preprocessed_patches_dir, patch_npz_name) # path to npz-patch\n",
        "\n",
        "#   with np.load(patch_npz_path) as data: # load npz-patch\n",
        "\n",
        "#     array_keys = list(data.keys()) # access to array in the npz-patch\n",
        "#     # print(f\"Array Keys: {array_keys}\")\n",
        "\n",
        "#     if len(array_keys) > 1: # if more than 1 array in the npz-patch\n",
        "#       print(f\"WARNING MESSAGE: .npz-file '{patch_npz_name}' contains {len(array_keys)} arrays: {array_keys}\") # Warning message\n",
        "\n",
        "#     patch_name = patch_npz_name.replace(\".npz\", \"\") # change to .npy-name\n",
        "#     array_data = data[array_keys[0]] # extract array data\n",
        "#     patches.append((patch_name, array_data)) # save as tuple of patch name and corresponding data in the patches list\n",
        "\n",
        "#     if idx % 100 == 0: # show progress\n",
        "#       print(f\"{idx}/{len(patches_npz_list)} files loaded.\")\n",
        "\n",
        "# print(f\"In total {len(patches)} patches successfully loaded.\")"
      ],
      "metadata": {
        "id": "J6VsG2C4kn2E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# FAST VERSION\n",
        "################################################################################\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # import ThreadPoolExecutor for parallelization (parallel loading of patches in each batch)\n",
        "\n",
        "# Final list to save loaded patches (name, data)\n",
        "patches = []\n",
        "\n",
        "# Function to load a single .npz file and extract the first array\n",
        "def load_npz_file(file_name, directory):\n",
        "    \"\"\" Load an .npz file, extract the first array, and return (name, array). \"\"\"\n",
        "    file_path = os.path.join(directory, file_name) # path to .npz-file\n",
        "\n",
        "    try:\n",
        "        with np.load(file_path) as data:  # Load the .npz file\n",
        "\n",
        "            array_keys = list(data.keys())  # Get all keys (array names)\n",
        "\n",
        "            if len(array_keys) > 1:  # Print a warning if multiple arrays are present\n",
        "                print(f\".npz-file '{file_name}' contains {len(array_keys)} arrays: {array_keys}\")\n",
        "\n",
        "            patch_name = file_name.replace(\".npz\", \"\")  # Remove '.npz' to get the base name\n",
        "            array_data = data[array_keys[0]]  # Extract the first array\n",
        "\n",
        "            return (patch_name, array_data)  # Return a tuple (name, array)\n",
        "\n",
        "    except Exception as e:  # Handle any errors during file loading\n",
        "        print(f\"Error loading {file_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Batch size to load files in chunks\n",
        "batch_size = 100\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel file loading\n",
        "for i in range(0, len(patches_npz_list), batch_size):\n",
        "    batch_files = patches_npz_list[i:i + batch_size]  # Get a batch of files\n",
        "    print(f\"Processing batch {i // batch_size + 1}/{(len(patches_npz_list) // batch_size) + 1}...\")\n",
        "\n",
        "    # List to temporarily store loaded patches from the current batch\n",
        "    batch_patches = []\n",
        "\n",
        "    # Parallel loading within the batch\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:  # Adjust max_workers as needed\n",
        "        futures = [executor.submit(load_npz_file, file_name, preprocessed_patches_dir) for file_name in batch_files]\n",
        "\n",
        "        # Collect results as they are completed\n",
        "        for future in as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result:  # Only append successful results\n",
        "                batch_patches.append(result)\n",
        "\n",
        "    patches.extend(batch_patches)  # Add the loaded batch to the final list\n",
        "    print(f\"Loaded {len(batch_patches)} patches in this batch.\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nIn total {len(patches)} patches successfully loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "KsafMPwRs6-E",
        "outputId": "1bd18d6d-a6d5-4a4b-a10a-244c4cbf3aa4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1/112...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b70e2bb7f914>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Collect results as they are completed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b70e2bb7f914>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Parallel loading within the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Adjust max_workers as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_npz_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_patches_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# patches"
      ],
      "metadata": {
        "id": "38-53s84nZi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test preprocessed patch\n",
        "################################################################################\n",
        "\n",
        "#####################\n",
        "SECTION = \"A01\" # -------------------->> ADJUSTABLE\n",
        "TEST_PATCH_ID = 16 # -------------------->> ADJUSTABLE\n",
        "#####################\n",
        "\n",
        "test_patch_path = preprocessed_patches_dir + f\"/{SECTION}_patch_{TEST_PATCH_ID}.npy.npz\"\n",
        "\n",
        "# load npz-file\n",
        "test_patch_npz = np.load(test_patch_path)\n",
        "# print(test_pre_patch_npz)\n",
        "\n",
        "for array in test_patch_npz.files:\n",
        "  print(array)\n",
        "  test_patch = test_patch_npz[array]\n",
        "  print(test_patch.shape)\n",
        "\n",
        "\n",
        "# extract the RGB image\n",
        "# test_pre_patch = test_pre_patch_npz['arr_0']\n",
        "# print(test_pre_patch)\n",
        "\n",
        "print(np.min(test_patch), np.max(test_patch))  # minimum and maximum\n",
        "test_patch_normalized = test_patch - np.min(test_patch ) # set minimum to 0\n",
        "test_patch_normalized = test_patch_normalized / np.max(test_patch_normalized)  # maximize to 1\n",
        "print(np.min(test_patch_normalized), np.max(test_patch_normalized))  # minimum and maximum after normalization\n",
        "\n",
        "\n",
        "# plot the preprocessed image\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(test_patch_normalized.transpose(1, 2, 0))  # transpose for RGB depiction\n",
        "ax.set_title(\"Preprocessed Patch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vhu2A-ZWNbev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Load masks\n",
        "################################################################################\n",
        "\n",
        "masks_list = [f for f in os.listdir(masks_dir) if f.endswith('_mask.npy')]  # list of all masks\n",
        "masks_list.sort() # sort list alphabetically\n",
        "# masks_list"
      ],
      "metadata": {
        "id": "2YXVuxh2x7sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test mask\n",
        "################################################################################\n",
        "\n",
        "#####################\n",
        "SECTION = \"A01\" # -------------------->> ADJUSTABLE\n",
        "TEST_MASK_ID = 96 # -------------------->> ADJUSTABLE\n",
        "#####################\n",
        "\n",
        "test_mask_path = masks_dir + f\"/{SECTION}_patch_{TEST_MASK_ID}_mask.npy\"\n",
        "\n",
        "# load mask\n",
        "test_mask = np.load(test_mask_path)\n",
        "\n",
        "# Plot:\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12))  # 3x3 grid (for 9 masks)\n",
        "axes = axes.flatten()  # easier to iterate through\n",
        "\n",
        "for i in range(test_mask.shape[0]):  # iterate through the 9 classes\n",
        "  axes[i].imshow(test_mask[i], cmap=\"gray\")\n",
        "  axes[i].set_title(f\"Class {i} - - {reversed_label_codes[i + 1]}\")\n",
        "  axes[i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OC_bi14-8mtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMstWKFge2b"
      },
      "source": [
        "## 2. Choose segmentation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_rZ-6Smge2b"
      },
      "outputs": [],
      "source": [
        "model = smp.Unet(   # -------------------->> ADJUSTABLE\n",
        "    encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=3,                  # model input channels (3 for RGB)\n",
        "    classes=len(label_codes),       # model output channels (number of classes)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToHcZmjUge2c"
      },
      "source": [
        "## 3. Splitting data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iBqoyNFvGuHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre_patches_list\n",
        "# masks_list"
      ],
      "metadata": {
        "id": "Kh83Xk9HHRiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper-function in order to extract section and patch_id\n",
        "def extract_section_and_id(file_name):\n",
        "    parts = file_name.split(\"_\") # split condition: _\n",
        "    section = parts[0]  # extract section from file_name, e.g. \"A01\"\n",
        "    patch_id = parts[2].replace(\".npy.npz\", \"\").replace(\"_mask\", \"\") #  extract patch_id, e.g. 481\n",
        "    return section, patch_id"
      ],
      "metadata": {
        "id": "cWF0Tdl1G1Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print section and patch_id from masks and patches\n",
        "print(extract_section_and_id(masks_list[1]))\n",
        "print(extract_section_and_id(patches_npz_list[175]))"
      ],
      "metadata": {
        "id": "pVNmOFytH_7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group patches by section\n",
        "section_patches = {} # empty dictionary\n",
        "for patch in patches_npz_list: # iterate over all preprocessed patches\n",
        "    section, patch_id = extract_section_and_id(patch) # extract section and id\n",
        "    section_patches.setdefault(section, []).append((patch, patch_id)) # creates keys of sections with patches and their ids inside"
      ],
      "metadata": {
        "id": "Xq6_p1FTQByr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(section_patches.keys())\n",
        "# print(section_patches)"
      ],
      "metadata": {
        "id": "yQs_gyIXrKUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate patches in Training and Validation/Test datasets by sections\n",
        "################################################################################\n",
        "### HYPERPARAMETER ####\n",
        "TRAIN_SECTIONS = [\"A01\", \"A02\", \"A03\", \"A04\", \"A05\", \"A08\"]  # Train sections\n",
        "TEST_SECTIONS = [\"A06\", \"A07\"]  # Validation/Test sections\n",
        "################################################################################\n",
        "\n",
        "# empty lists for patches\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for section, files in section_patches.items(): # iterates through the dictionary of sections\n",
        "    if section in TRAIN_SECTIONS: # if the section is a training section\n",
        "        train_data.extend(files) # if yes, the patches are added to the training data\n",
        "    elif section in TEST_SECTIONS:\n",
        "        test_data.extend(files)\n",
        "\n",
        "print(f\"Training Patches: {len(train_data)}\")\n",
        "print(f\"Test Patches: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "o-DUEfQCQ_DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[8][0]"
      ],
      "metadata": {
        "id": "NXoQMH6GB8Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_mask(patch_name, masks_dir):\n",
        "    mask_path = os.path.join(masks_dir, patch_name.replace(\".npy.npz\", \"_mask.npy\"))\n",
        "    return os.path.exists(mask_path)"
      ],
      "metadata": {
        "id": "1kwq3e4W-Hj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_with_masks = [f for f in train_data if has_mask(f[0], masks_dir)]\n",
        "train_background = [f for f in train_data if not has_mask(f[0],masks_dir)]\n",
        "test_with_masks = [f for f in test_data if has_mask(f[0], masks_dir)]\n",
        "test_background = [f for f in test_data if not has_mask(f[0], masks_dir)]\n",
        "\n",
        "print(f\"Training mit Masken: {len(train_with_masks)}\")\n",
        "print(f\"Training Hintergrund: {len(train_background)}\")\n",
        "print(f\"Test mit Masken: {len(test_with_masks)}\")\n",
        "print(f\"Test Hintergrund: {len(test_background)}\")"
      ],
      "metadata": {
        "id": "NgAZUMH5Am0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PatchDataset(Dataset):\n",
        "    def __init__(self, data_list, patches_dir, masks_dir=None, transform=None):\n",
        "        \"\"\"\n",
        "        Custom Dataset for loading patches and optional masks.\n",
        "        Args:\n",
        "            data_list (list): List of (patch_name, array (image data)) tuples.\n",
        "            patches_dir (str): Directory containing patch .npy files.\n",
        "            masks_dir (str): Directory containing mask.npy files (optional).\n",
        "            transform (callable, optional): Transformation to be applied to the data.\n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "        self.patches_dir = patches_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the patch\n",
        "        patch_name = self.data_list[idx][0]\n",
        "        patch_path = os.path.join(self.patches_dir, patch_name)\n",
        "        patch = np.load(patch_path)\n",
        "\n",
        "        # Convert to Tensor and change dtype\n",
        "        patch = torch.tensor(patch, dtype=torch.float32)\n",
        "\n",
        "        # Load the mask if available\n",
        "        if self.masks_dir:\n",
        "            mask_path = os.path.join(self.masks_dir, patch_name.replace(\".npy.npz\", \"_mask.npy\"))\n",
        "            if os.path.exists(mask_path):\n",
        "                mask = np.load(mask_path)\n",
        "                mask = torch.tensor(mask, dtype=torch.float32)\n",
        "            else:\n",
        "                mask = torch.zeros((patch.shape[1], patch.shape[2]), dtype=torch.float32)  # Background mask\n",
        "        else:\n",
        "            mask = torch.zeros((patch.shape[1], patch.shape[2]), dtype=torch.float32)  # Default background mask\n",
        "\n",
        "        # Apply any transformations if needed\n",
        "        if self.transform:\n",
        "            patch, mask = self.transform(patch, mask)\n",
        "\n",
        "        return patch, mask\n"
      ],
      "metadata": {
        "id": "o-O42wN-3JR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC2n49Xgge2c"
      },
      "source": [
        "### ?.1 Choose optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WzJiOHarge2c"
      },
      "outputs": [],
      "source": [
        "loss = smp.losses.DiceLoss(mode=\"multiclass\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citing"
      ],
      "metadata": {
        "id": "M3aTlcGv09d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @misc{Iakubovskii:2019,\n",
        "#   Author = {Pavel Iakubovskii},\n",
        "#   Title = {Segmentation Models Pytorch},\n",
        "#   Year = {2019},\n",
        "#   Publisher = {GitHub},\n",
        "#   Journal = {GitHub repository},\n",
        "#   Howpublished = {\\url{https://github.com/qubvel/segmentation_models.pytorch}}\n",
        "# }"
      ],
      "metadata": {
        "id": "Hu7b1o7d1E_q"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}