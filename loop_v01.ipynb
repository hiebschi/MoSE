{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiebschi/MoSE_scripts/blob/main/loop_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQ_Gx_rrKju"
      },
      "source": [
        "\n",
        "# Implementing a Segmentation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMZ_BMRnge2X"
      },
      "source": [
        "## 1. Preparations\n",
        "### 1.1 Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZhEfhwXBge2Y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import sklearn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "id": "yCfa0JGpElrw",
        "outputId": "05f95155-5d6e-401d-ac99-caf742bdf903",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "torchvision version: 0.20.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Install segmentation model"
      ],
      "metadata": {
        "id": "3RTROJOlxwZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAdkr3mEge2a",
        "outputId": "ae90e9cb-8a6e-46d8-e77c-68a46d23a3f6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.26.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (11.0.0)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.17.0)\n",
            "Requirement already satisfied: timm==0.9.7 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.9.7)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.5.1+cu121)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xI34Hl2ge2a"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Device agnostic code"
      ],
      "metadata": {
        "id": "paG6kc2OyBYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "8whgTmUHyEq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1Db8WbBge2a"
      },
      "source": [
        "### 1.4 Upload data to colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manual upload (for small files)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FwCYsWKkDZKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual upload through Files Tab (right Tab)\n",
        "!ls /content"
      ],
      "metadata": {
        "id": "Hr_ojI0XLHuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCESS TO GOOGLE DRIVE\n",
        "################################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8FTZJtueDoWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_bWdgKqge2a"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Data directories on GOOGLE DRIVE  # -------------------->> ADJUSTABLE\n",
        "################################################################################\n",
        "# npy preprocessed patches\n",
        "preprocessed_patches_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/preprocessed_patches'\n",
        "# JSON class codes\n",
        "codes_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/classes'\n",
        "# Shapefile class labels\n",
        "labels_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/shapefiles'\n",
        "# npy Masks\n",
        "masks_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/masks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1kdiZmage2b"
      },
      "outputs": [],
      "source": [
        "# Load class labels\n",
        "################################################################################\n",
        "# specific Shapefile path\n",
        "shp_path = os.path.join(labels_dir, \"GSK_24_WGS84_adjusted.shp\") # -------------------->> ADJUSTABLE\n",
        "\n",
        "labels = gpd.read_file(shp_path) # read shapefile\n",
        "labels_filtered = labels[labels[\"Elementtyp\"].notnull()] # remove NULL\n",
        "print(labels_filtered[\"Elementtyp\"].unique()) # print all label classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFxyHS__ge2b"
      },
      "outputs": [],
      "source": [
        "# Load class codes\n",
        "################################################################################\n",
        "# path\n",
        "label_codes_path = os.path.join(codes_dir, \"label_codes.json\")   # -------------------->> ADJUSTABLE\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(label_codes_path, \"r\") as json_file:\n",
        "    label_codes = json.load(json_file)\n",
        "\n",
        "print(label_codes)\n",
        "len(label_codes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reversed dictionary\n",
        "reversed_label_codes = {v: k for k, v in label_codes.items()} # v = value, k = key\n",
        "reversed_label_codes[4]"
      ],
      "metadata": {
        "id": "CtVjFKrwEJht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Load preprocessed patches\n",
        "################################################################################\n",
        "\n",
        "# list of all .npz-files (.npz-compression)\n",
        "################################################################################\n",
        "patches_npz_list = [f for f in os.listdir(preprocessed_patches_dir) if f.endswith('.npz')]\n",
        "patches_npz_list.sort()\n",
        "patches_npz_list[0:3]"
      ],
      "metadata": {
        "id": "ISqNiRoew3wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ################################################################################\n",
        "# # SLOW VERSION TO LOAD ALL PATCHES (arrays)\n",
        "# ################################################################################\n",
        "\n",
        "# # empty list for saving loaded arrays (patches)\n",
        "# patches = []\n",
        "\n",
        "# for idx, patch_npz_name in enumerate(patches_npz_list): # iterates over all npz-patches in the list\n",
        "#   patch_npz_path = os.path.join(preprocessed_patches_dir, patch_npz_name) # path to npz-patch\n",
        "\n",
        "#   with np.load(patch_npz_path) as data: # load npz-patch\n",
        "\n",
        "#     array_keys = list(data.keys()) # access to array in the npz-patch\n",
        "#     # print(f\"Array Keys: {array_keys}\")\n",
        "\n",
        "#     if len(array_keys) > 1: # if more than 1 array in the npz-patch\n",
        "#       print(f\"WARNING MESSAGE: .npz-file '{patch_npz_name}' contains {len(array_keys)} arrays: {array_keys}\") # Warning message\n",
        "\n",
        "#     patch_name = patch_npz_name.replace(\".npz\", \"\") # change to .npy-name\n",
        "#     array_data = data[array_keys[0]] # extract array data\n",
        "#     patches.append((patch_name, array_data)) # save as tuple of patch name and corresponding data in the patches list\n",
        "\n",
        "#     if idx % 100 == 0: # show progress\n",
        "#       print(f\"{idx}/{len(patches_npz_list)} files loaded.\")\n",
        "\n",
        "# print(f\"In total {len(patches)} patches successfully loaded.\")"
      ],
      "metadata": {
        "id": "J6VsG2C4kn2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ################################################################################\n",
        "# # FAST VERSION\n",
        "# ################################################################################\n",
        "\n",
        "# from concurrent.futures import ThreadPoolExecutor, as_completed # import ThreadPoolExecutor for parallelization (parallel loading of patches in each batch)\n",
        "\n",
        "# # Final list to save loaded patches (name, data)\n",
        "# patches = []\n",
        "\n",
        "# # Function to load a single .npz file and extract the first array\n",
        "# def load_npz_file(file_name, directory):\n",
        "#     \"\"\" Load an .npz file, extract the first array, and return (name, array). \"\"\"\n",
        "#     file_path = os.path.join(directory, file_name) # path to .npz-file\n",
        "\n",
        "#     try:\n",
        "#         with np.load(file_path) as data:  # Load the .npz file\n",
        "\n",
        "#             array_keys = list(data.keys())  # Get all keys (array names)\n",
        "\n",
        "#             if len(array_keys) > 1:  # Print a warning if multiple arrays are present\n",
        "#                 print(f\".npz-file '{file_name}' contains {len(array_keys)} arrays: {array_keys}\")\n",
        "\n",
        "#             patch_name = file_name.replace(\".npz\", \"\")  # Remove '.npz' to get the base name\n",
        "#             array_data = data[array_keys[0]]  # Extract the first array\n",
        "\n",
        "#             return (patch_name, array_data)  # Return a tuple (name, array)\n",
        "\n",
        "#     except Exception as e:  # Handle any errors during file loading\n",
        "#         print(f\"Error loading {file_name}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Batch size to load files in chunks\n",
        "# batch_size = 100\n",
        "\n",
        "# # Use ThreadPoolExecutor for parallel file loading\n",
        "# for i in range(0, len(patches_npz_list), batch_size):\n",
        "#     batch_files = patches_npz_list[i:i + batch_size]  # Get a batch of files\n",
        "#     print(f\"Processing batch {i // batch_size + 1}/{(len(patches_npz_list) // batch_size) + 1}...\")\n",
        "\n",
        "#     # List to temporarily store loaded patches from the current batch\n",
        "#     batch_patches = []\n",
        "\n",
        "#     # Parallel loading within the batch\n",
        "#     with ThreadPoolExecutor(max_workers=8) as executor:  # Adjust max_workers as needed\n",
        "#         futures = [executor.submit(load_npz_file, file_name, preprocessed_patches_dir) for file_name in batch_files]\n",
        "\n",
        "#         # Collect results as they are completed\n",
        "#         for future in as_completed(futures):\n",
        "#             result = future.result()\n",
        "#             if result:  # Only append successful results\n",
        "#                 batch_patches.append(result)\n",
        "\n",
        "#     patches.extend(batch_patches)  # Add the loaded batch to the final list\n",
        "#     print(f\"Loaded {len(batch_patches)} patches in this batch.\")\n",
        "\n",
        "# # Summary\n",
        "# print(f\"\\nIn total {len(patches)} patches successfully loaded.\")"
      ],
      "metadata": {
        "id": "KsafMPwRs6-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test preprocessed patch\n",
        "################################################################################\n",
        "\n",
        "#####################\n",
        "SECTION = \"A01\" # -------------------->> ADJUSTABLE\n",
        "TEST_PATCH_ID = 16 # -------------------->> ADJUSTABLE\n",
        "#####################\n",
        "\n",
        "test_patch_path = preprocessed_patches_dir + f\"/{SECTION}_patch_{TEST_PATCH_ID}.npy.npz\"\n",
        "\n",
        "# load npz-file\n",
        "test_patch_npz = np.load(test_patch_path)\n",
        "# print(test_pre_patch_npz)\n",
        "\n",
        "for array in test_patch_npz.files:\n",
        "  print(array)\n",
        "  test_patch = test_patch_npz[array]\n",
        "  print(test_patch.shape)\n",
        "\n",
        "# extract the RGB image\n",
        "# test_pre_patch = test_pre_patch_npz['arr_0']\n",
        "# print(test_pre_patch)\n",
        "\n",
        "print(np.min(test_patch), np.max(test_patch))  # minimum and maximum\n",
        "test_patch_normalized = test_patch - np.min(test_patch ) # set minimum to 0\n",
        "test_patch_normalized = test_patch_normalized / np.max(test_patch_normalized)  # maximize to 1\n",
        "print(np.min(test_patch_normalized), np.max(test_patch_normalized))  # minimum and maximum after normalization\n",
        "\n",
        "\n",
        "# plot the preprocessed image\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(test_patch_normalized.transpose(1, 2, 0))  # transpose for RGB depiction\n",
        "ax.set_title(\"Preprocessed Patch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vhu2A-ZWNbev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Load masks\n",
        "################################################################################\n",
        "\n",
        "masks_list = [f for f in os.listdir(masks_dir) if f.endswith('_mask.npy')]  # list of all masks\n",
        "masks_list.sort() # sort list alphabetically\n",
        "# masks_list"
      ],
      "metadata": {
        "id": "2YXVuxh2x7sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test mask\n",
        "################################################################################\n",
        "\n",
        "#####################\n",
        "SECTION = \"A01\" # -------------------->> ADJUSTABLE\n",
        "TEST_MASK_ID = 96 # -------------------->> ADJUSTABLE\n",
        "#####################\n",
        "\n",
        "test_mask_path = masks_dir + f\"/{SECTION}_patch_{TEST_MASK_ID}_mask.npy\"\n",
        "\n",
        "# load mask\n",
        "test_mask = np.load(test_mask_path)\n",
        "\n",
        "# Plot:\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12))  # 3x3 grid (for 9 masks)\n",
        "axes = axes.flatten()  # easier to iterate through\n",
        "\n",
        "for i in range(test_mask.shape[0]):  # iterate through the 9 classes\n",
        "  axes[i].imshow(test_mask[i], cmap=\"gray\")\n",
        "  axes[i].set_title(f\"Class {i} - - {reversed_label_codes[i + 1]}\")\n",
        "  axes[i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OC_bi14-8mtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMstWKFge2b"
      },
      "source": [
        "## 2. Choose segmentation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_rZ-6Smge2b"
      },
      "outputs": [],
      "source": [
        "model = smp.Unet(   # -------------------->> ADJUSTABLE\n",
        "    encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=3,                  # model input channels (3 for RGB)\n",
        "    classes=len(label_codes),       # model output channels (number of classes)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToHcZmjUge2c"
      },
      "source": [
        "## 3. Splitting data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iBqoyNFvGuHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre_patches_list\n",
        "# masks_list"
      ],
      "metadata": {
        "id": "Kh83Xk9HHRiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper-function in order to extract section and patch_id\n",
        "def extract_section_and_id(file_name):\n",
        "    parts = file_name.split(\"_\") # split condition: _\n",
        "    section = parts[0]  # extract section from file_name, e.g. \"A01\"\n",
        "    patch_id = parts[2].replace(\".npy.npz\", \"\").replace(\"_mask\", \"\") #  extract patch_id, e.g. 481\n",
        "    return section, patch_id"
      ],
      "metadata": {
        "id": "cWF0Tdl1G1Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print section and patch_id from masks and patches\n",
        "print(extract_section_and_id(masks_list[1]))\n",
        "print(extract_section_and_id(patches_npz_list[175]))"
      ],
      "metadata": {
        "id": "pVNmOFytH_7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group patches by section\n",
        "section_patches = {} # empty dictionary\n",
        "for patch in patches_npz_list: # iterate over all preprocessed patches\n",
        "    section, patch_id = extract_section_and_id(patch) # extract section and id\n",
        "    section_patches.setdefault(section, []).append((patch)) # creates keys of sections with their patches inside"
      ],
      "metadata": {
        "id": "Xq6_p1FTQByr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(section_patches.keys())\n",
        "# print(section_patches)"
      ],
      "metadata": {
        "id": "yQs_gyIXrKUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate patches in Training and Validation/Test datasets by sections\n",
        "################################################################################\n",
        "### HYPERPARAMETER ####\n",
        "TRAIN_SECTIONS = [\"A01\", \"A02\", \"A03\", \"A04\", \"A05\", \"A08\"]  # Train sections\n",
        "TEST_SECTIONS = [\"A06\", \"A07\"]  # Validation/Test sections\n",
        "################################################################################\n",
        "\n",
        "# empty lists for patches\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for section, files in section_patches.items(): # iterates through the dictionary of sections\n",
        "    if section in TRAIN_SECTIONS: # if the section is a training section\n",
        "        train_data.extend(files) # if yes, the patches are added to the training data\n",
        "    elif section in TEST_SECTIONS:\n",
        "        test_data.extend(files)\n",
        "\n",
        "print(f\"Training Patches: {len(train_data)}\")\n",
        "print(f\"Test Patches: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "o-DUEfQCQ_DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[8]"
      ],
      "metadata": {
        "id": "NXoQMH6GB8Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_mask(patch_name, masks_dir):\n",
        "    mask_path = os.path.join(masks_dir, patch_name.replace(\".npy.npz\", \"_mask.npy\"))\n",
        "    return os.path.exists(mask_path)\n",
        "\n",
        "has_mask(train_data[8], masks_dir)"
      ],
      "metadata": {
        "id": "1kwq3e4W-Hj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_with_masks = [f for f in train_data if has_mask(f, masks_dir)]\n",
        "# print(train_with_masks)\n",
        "train_background = [f for f in train_data if not has_mask(f, masks_dir)]\n",
        "test_with_masks = [f for f in test_data if has_mask(f, masks_dir)]\n",
        "test_background = [f for f in test_data if not has_mask(f, masks_dir)]\n",
        "\n",
        "print(f\"Training mit Masken: {len(train_with_masks)}\")\n",
        "print(f\"Training Hintergrund: {len(train_background)}\")\n",
        "print(f\"Test mit Masken: {len(test_with_masks)}\")\n",
        "print(f\"Test Hintergrund: {len(test_background)}\")"
      ],
      "metadata": {
        "id": "NgAZUMH5Am0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load a single .npz file and extract the first array\n",
        "def load_npz_patch(patch_npz_name, patches_npz_dir):\n",
        "\n",
        "    \"\"\" Load patch .npz file, extract the first array and return it.\n",
        "    Output shape: (3, 512, 512) \"\"\"\n",
        "\n",
        "    patch_npz_path = os.path.join(patches_npz_dir, patch_npz_name) # path to .npz-file\n",
        "\n",
        "    try:\n",
        "        with np.load(patch_npz_path) as data:  # Load the patch .npz-file\n",
        "\n",
        "            array_keys = list(data.keys())  # Get all keys (array names)\n",
        "\n",
        "            if len(array_keys) > 1:  # Print a warning if multiple arrays are present\n",
        "                print(f\".npz-file '{patch_npz_name}' contains {len(array_keys)} arrays: {array_keys}\")\n",
        "\n",
        "            patch_name = patch_npz_name.replace(\".npz\", \"\")  # Remove '.npz' to get the base name\n",
        "\n",
        "            patch_image = data[array_keys[0]]  # Extract the first array\n",
        "\n",
        "\n",
        "            return (patch_name, patch_image)\n",
        "\n",
        "    except Exception as e:  # Handle any errors during file loading\n",
        "        print(f\"Error loading {patch_npz_name}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ScoGhbJsR-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PatchDataset(Dataset):\n",
        "    def __init__(self, patches_npz_list, patches_npz_dir, masks_dir=None, transform=None):\n",
        "        \"\"\"\n",
        "        Custom Dataset for loading .npz patches and optional masks.\n",
        "        Args:\n",
        "            patches_npz_list (list): List of the patch .npz-files.\n",
        "            patches_npz_dir (str): Directory containing patch .npz-files.\n",
        "            masks_dir (str): Directory containing mask.npy files (optional).\n",
        "            transform (callable, optional): Transformation to be applied to the data.\n",
        "        \"\"\"\n",
        "\n",
        "        self.patches_npz_list = patches_npz_list\n",
        "        self.patches_npz_dir = patches_npz_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patches_npz_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract name and path of a single patch\n",
        "        patch_npz_name = self.patches_npz_list[idx]\n",
        "\n",
        "        patch = load_npz_patch(patch_npz_name, self.patches_npz_dir)\n",
        "        patch_name, patch = patch\n",
        "\n",
        "        # Convert to Tensor and change dtype\n",
        "        patch = torch.tensor(patch, dtype=torch.float32)\n",
        "\n",
        "        # Load the mask if available\n",
        "        if self.masks_dir:\n",
        "            mask_path = os.path.join(self.masks_dir, patch_name.replace(\".npy\", \"_mask.npy\"))\n",
        "            if os.path.exists(mask_path):\n",
        "                mask = np.load(mask_path)\n",
        "                mask = torch.tensor(mask, dtype=torch.long)\n",
        "            else:\n",
        "                mask = torch.zeros((patch.shape[1], patch.shape[2]), dtype=torch.long)  # Background mask\n",
        "        else:\n",
        "            mask = torch.zeros((patch.shape[1], patch.shape[2]), dtype=torch.long)  # Default background mask\n",
        "\n",
        "        # Apply any transformations if needed\n",
        "        if self.transform:\n",
        "            patch, mask = self.transform(patch, mask)\n",
        "\n",
        "        return patch, mask\n"
      ],
      "metadata": {
        "id": "o-O42wN-3JR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Datasets\n",
        "train_dataset = PatchDataset(train_with_masks, preprocessed_patches_dir, masks_dir)\n",
        "print(train_dataset)\n",
        "train_background_dataset = PatchDataset(train_background, preprocessed_patches_dir)\n",
        "\n",
        "test_dataset = PatchDataset(test_with_masks, preprocessed_patches_dir, masks_dir)\n",
        "test_background_dataset = PatchDataset(test_background, preprocessed_patches_dir)\n",
        "\n",
        "# Combine Masked and Background datasets into one DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=2)\n",
        "print(train_loader)\n",
        "train_background_loader = DataLoader(train_background_dataset, batch_size=10, shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=2)\n",
        "test_background_loader = DataLoader(test_background_dataset, batch_size=10, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Number of Training Batches: {len(train_loader)}\")\n",
        "print(f\"Number of Test Batches: {len(test_loader)}\")\n"
      ],
      "metadata": {
        "id": "9Ld2FgTWOfrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check one batch of data\n",
        "for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\"  Images shape: {images.shape}\")  # Should be [batch_size, channels, height, width]\n",
        "    print(f\"  Masks shape: {masks.shape}\")    # Should be [batch_size, channels height, width]\n",
        "    break"
      ],
      "metadata": {
        "id": "mGfD8AZHPej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC2n49Xgge2c"
      },
      "source": [
        "### ?.1 Choose optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzJiOHarge2c"
      },
      "outputs": [],
      "source": [
        "loss_fn = smp.losses.DiceLoss(mode=\"multiclass\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 5  # Adjustable number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "        images, masks = images.to(device), masks.to(device)  # Move to GPU if available\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)  # Predictions\n",
        "        loss = loss_fn(outputs, masks)  # Calculate loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:  # Display progress every 10 batches\n",
        "            print(f\"Batch {batch_idx + 1}/{len(train_loader)}: Loss = {loss.item()}\")\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation during validation\n",
        "        for batch_idx, (images, masks) in enumerate(test_loader):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    print(\"-\" * 50)  # Separator for better readability\n"
      ],
      "metadata": {
        "id": "d0eSNZufb_rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citing"
      ],
      "metadata": {
        "id": "M3aTlcGv09d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @misc{Iakubovskii:2019,\n",
        "#   Author = {Pavel Iakubovskii},\n",
        "#   Title = {Segmentation Models Pytorch},\n",
        "#   Year = {2019},\n",
        "#   Publisher = {GitHub},\n",
        "#   Journal = {GitHub repository},\n",
        "#   Howpublished = {\\url{https://github.com/qubvel/segmentation_models.pytorch}}\n",
        "# }"
      ],
      "metadata": {
        "id": "Hu7b1o7d1E_q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}