{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZQ_Gx_rrKju"
   },
   "source": [
    "\n",
    "# Implementing a Segmentation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations\n",
    "### 1.1 Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sklearn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\pyth3130_24\\Lib\\site-packages\\segmentation_models_pytorch\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Unet\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinknet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linknet\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfpn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FPN\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\pyth3130_24\\Lib\\site-packages\\segmentation_models_pytorch\\unet\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Unet\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\pyth3130_24\\Lib\\site-packages\\segmentation_models_pytorch\\unet\\model.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnetDecoder\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_encoder\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\pyth3130_24\\Lib\\site-packages\\segmentation_models_pytorch\\unet\\decoder.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Upload data to colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual upload (for small files)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directories on GOOGLE DRIVE  # -------------------->> ADJUSTABLE\n",
    "# npy patches\n",
    "patches_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/patches'\n",
    "# JSON class codes\n",
    "codes_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/classes'\n",
    "# Shapefile class labels\n",
    "labels_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/shapefiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels\n",
    "# specific Shapefile path\n",
    "shp_path = os.path.join(labels_dir, \"GSK_24_WGS84_adjusted.shp\") # -------------------->> ADJUSTABLE\n",
    "\n",
    "labels = gpd.read_file(shp_path) # read shapefile\n",
    "labels_filtered = labels[labels[\"Elementtyp\"].notnull()] # remove NULL\n",
    "print(labels_filtered[\"Elementtyp\"].unique()) # print all label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Einzelstein': 1, 'Wurzelstock': 2, 'Steinverbauung': 3, 'Totholz': 4, 'Steinriegel': 5, 'Schotterbank': 6, 'Schlamm_Sandinsel': 7, 'Sand_Schlammbank': 8, 'Schotterinsel': 9}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class codes\n",
    "# path\n",
    "label_codes_path = os.path.join(codes_dir, \"label_codes.json\")   # -------------------->> ADJUSTABLE\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(label_codes_path, \"r\") as json_file:\n",
    "    label_codes = json.load(json_file)\n",
    "\n",
    "print(label_codes)\n",
    "len(label_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241m.\u001b[39mUnet(\n\u001b[0;32m      2\u001b[0m     encoder_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet34\u001b[39m\u001b[38;5;124m\"\u001b[39m,        \u001b[38;5;66;03m# choose encoder, e.g. mobilenet_v2 or efficientnet-b7\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     encoder_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;66;03m# use `imagenet` pre-trained weights for encoder initialization\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,                  \u001b[38;5;66;03m# model input channels (1 for gray-scale images, 3 for RGB, etc.)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_codes),                      \u001b[38;5;66;03m# model output channels (number of classes in your dataset)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'smp' is not defined"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(   # -------------------->> ADJUSTABLE\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (3 for RGB)\n",
    "    classes=len(label_codes),       # model output channels (number of classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure data preprocessing\n",
    "\n",
    "Datenvorverarbeitungsfunktion (preprocess_input) für ein bestimmtes Modell (Encoder) konfigurieren:\n",
    "\n",
    "`from segmentation_models_pytorch.encoders import get_preprocessing_fn`\n",
    "\n",
    "`preprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')`\n",
    "\n",
    "Encoder = der Teil des neuronalen Netzes, der Features aus den Eingabedaten extrahiert (in diesem Fall das vortrainiertes Modell \"resnet\" auf ImageNet)\n",
    "Pretraining = Encoder bereits auf einem großen Datensatz (z. B. ImageNet) trainiert > kann nützliche feature erkennen\n",
    "\n",
    "Warum Datenvorverarbeitung nötig ist:\n",
    "- Während des Trainings der vortrainierten Gewichte (z. B. auf ImageNet) wurde eine spezifische Vorverarbeitung auf die Bilder angewendet, z. B. Normalisierung, Größenanpassung oder Farbraumkonvertierung.\n",
    "- Um die bestmögliche Leistung mit den vortrainierten Gewichten zu erzielen, sollten die Eingabedaten genauso vorverarbeitet werden, wie es während des Pretrainings gemacht wurde.\n",
    "\n",
    "Was `get_preprocessing_fn` macht:\n",
    "Diese Funktion liefert eine Vorverarbeitungsfunktion (preprocess_input), die genau die gleichen Transformationen durchführt, die während des Pretrainings auf den ImageNet-Datensatz angewendet wurden.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "preprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet') # load specific (suitable to chosen model) preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directory for preprocessed patches\n",
    "preprocessed_patches_dir = '/content/drive/My Drive/Dokumente.GD/FS06 SS24/BACHELORARBEIT/MoSE/data/preprocessed_patches'\n",
    "if not os.path.exists(preprocessed_patches_dir):\n",
    "    os.makedirs(preprocessed_patches_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_list = [f for f in os.listdir(patches_dir) if not f.endswith('_mask.npy')]  # list of all patches (not the masks)\n",
    "patches_list.sort() # sort list alphabetically\n",
    "patches_list\n",
    "\n",
    "for idx, patch_name in enumerate(patches_list):\n",
    "    patch_path = os.path.join(patches_dir, patch_name)  # path to specific patch\n",
    "    patch = np.load(patch_path)  # load npy file (Shape: [Channels, Height, Width])\n",
    "\n",
    "# HIEEER WEITER\n",
    "    # Überprüfe das Shape und transponiere, falls nötig\n",
    "    if patch.shape[0] in [1, 3]:  # Annahme: Shape ist [C, H, W] oder [H, W, C]\n",
    "        patch = patch.transpose(1, 2, 0)  # [H, W, C] für preprocess_input\n",
    "\n",
    "    # Wende die Preprocessing-Funktion an\n",
    "    preprocessed_patch = preprocess_input(patch)\n",
    "\n",
    "    # Transponiere zurück zu [C, H, W], wenn nötig\n",
    "    preprocessed_patch = preprocessed_patch.transpose(2, 0, 1)\n",
    "\n",
    "    # Speichere das vorverarbeitete Patch\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "    np.save(output_path, preprocessed_patch)\n",
    "\n",
    "    # Fortschritt ausgeben\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(file_list)} patches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write training and  testing loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Choose optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.losses.DiceLoss(mode=\"multiclass\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNvfmeo4e4XEBxtALEWuyJt",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyth3130_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
